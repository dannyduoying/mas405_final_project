---
title: "dm_mas405_final"
author: "Dominique McDonald"
date: '2022-05-18'
output: pdf_document
---

Using the following as a guide: https://www.red-gate.com/simple-talk/databases/sql-server/bi-sql-server/text-mining-and-sentiment-analysis-with-r/




Installing packages for text analysis.
```{r}
# install.packages("tm")  # for text mining
# install.packages("SnowballC") # for text stemming
# install.packages("wordcloud") # word-cloud generator
# install.packages("RColorBrewer") # color palettes
# install.packages("syuzhet") # for sentiment analysis
# install.packages("ggplot2")
```

```{r, warning= FALSE}
#Now load those bad boys


library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
library(RMySQL)
```





```{r}
#connecting to my db

drv <- dbDriver("MySQL")
xdbsock <- ""


#############
xdbuser <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_USER")
xpw     <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_PW")
xdbname <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_DBNAME")
xdbhost <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_HOST")
xdbport <- as.integer( Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_PORT") )



con <-
dbConnect(
drv,
user=xdbuser,
password=xpw,
dbname=xdbname,
host=xdbhost,
port=xdbport,
unix.sock=xdbsock
)


dbListTables(con)

```







```{r}
#getting the data

qry1 <- "SELECT * FROM sonnets"

x <- dbGetQuery(con, qry1)

head(x$Sonnets)



```


```{r}
#need to load vector of text objects as a corpus
#VectorSource() interprets each element of a vec as a document
x_text <- Corpus(VectorSource(x$Sonnets))


x_text
```
In the future no need to use content_transformer(). That is easy to do in base and it drops documents(???) 
```{r}
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (y , pattern ) gsub(pattern, " ", y))
#to_e <- content_transformer(function (y , pattern ) gsub(pattern, "e", y)) wasnt' helpful
x_text <- tm_map(x_text, toSpace, "/")
x_text <- tm_map(x_text, toSpace, "@")
x_text <- tm_map(x_text, toSpace, "\\|")
x_text <- tm_map(x_text, to_e, "â€™")
# Convert the text to lower case
#x_text <- tm_map(x_text, content_transformer(tolower))
# Remove numbers
x_text <- tm_map(x_text, removeNumbers)
# Remove english common stopwords
x_text <- tm_map(x_text, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector

x_text <- tm_map(x_text, removeWords, c("thi", "thee", "thou", "may", "still", "thus", "though", "can", "will", "hath", "doth", "thine", "like", "much", "let",
                                        "upon", "from", "dost", "shall", "thy")) 
# Remove punctuations
x_text <- tm_map(x_text, removePunctuation)
# Eliminate extra white spaces
x_text <- tm_map(x_text, stripWhitespace)
# Text stemming - which reduces words to their root form
x_text <- tm_map(x_text, stemDocument)

x_text #NO DOCUMENTS DROPPED

```
The document term matrix just contains all the words in your "documents" and their frequencies and maybe their stem, gotta check
```{r}
# Build a term-document matrix
x_text_dtm <- TermDocumentMatrix(x_text)
mat_dtm <- as.matrix(x_text_dtm)
# mat_dtm
# Sort by decreasing value of frequency
dtm_v <- sort(rowSums(mat_dtm),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 50)


```

```{r}
#generate word cloud
set.seed(314)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

##play around with shapes

```
```{r}
# Find associations 
findAssocs(x_text_dtm, terms = c("beauty","sweet","love", "heart", "time"), corlimit = 0.1)			




```


```{r}
# Find associations for words that occur at least 50 times
findAssocs(x_text_dtm, terms = findFreqTerms(x_text_dtm, lowfreq = 850), corlimit = 0.1)



```

```{r}

# regular sentiment score using get_sentiment() function and method of your choice
# please note that different methods may have different scales
s_vector <- get_sentiment(x$Sonnets, method="syuzhet")
# see the first row of the vector
head(s_vector, n = 10)
# see summary statistics of the vector
summary(s_vector)
hist(s_vector, col = "pink") #normal distribution 

```





Looking into other data sets
```{r}

qry1 <- "SELECT * FROM netflix"

netflix <- dbGetQuery(con, qry1)

head(netflix)
names(netflix)
head(netflix$description)#probably more useful
head(netflix$genres) #not gonna be helpful for us really

```




```{r}
#need to load vector of text objects as a corpus
#VectorSource() interprets each element of a vec as a document
netflix_text <- Corpus(VectorSource(netflix$description))


netflix_text
```



In the future no need to use content_transformer(). That is easy to do in base and it drops documents(???) 
```{r}
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (y , pattern ) gsub(pattern, " ", y))
#to_e <- content_transformer(function (y , pattern ) gsub(pattern, "e", y)) wasnt' helpful
netflix_text <- tm_map(netflix_text, toSpace, "/")
netflix_text <- tm_map(netflix_text, toSpace, "@")
netflix_text <- tm_map(netflix_text, toSpace, "\\|")
netflix_text <- tm_map(netflix_text, to_e, "â€™")
# Convert the text to lower case
netflix_text <- tm_map(netflix_text, content_transformer(tolower))
# Remove numbers
netflix_text <- tm_map(netflix_text, removeNumbers)
# Remove english common stopwords
netflix_text <- tm_map(netflix_text, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector

netflix_text <- tm_map(netflix_text, removeWords, c("thi", "thee", "thou", "may", "still", "thus", "though", "can", "will", "hath", "doth", "thine", "like", "much", "let",
                                        "upon", "from", "dost", "shall", "thy")) 
# Remove punctuations
netflix_text <- tm_map(netflix_text, removePunctuation)
# Eliminate extra white spaces
netflix_text <- tm_map(netflix_text, stripWhitespace)
# Text stemming - which reduces words to their root form
#x_text <- tm_map(x_text, stemDocument)

netflix_text #NO DOCUMENTS DROPPED

```
```{r}
# Build a term-document matrix
netflix_text_dtm <- TermDocumentMatrix(netflix_text)
netflix_mat_dtm <- as.matrix(netflix_text_dtm)
#head(netflix_mat_dtm)
# Sort by decreasing value of frequency
netflix_dtm_v <- sort(rowSums(netflix_mat_dtm),decreasing=TRUE)
netflix_dtm_d <- data.frame(word = names(netflix_dtm_v),freq=netflix_dtm_v)
# Display the top 5 most frequent words
head(netflix_dtm_d, 50)


```






```{r}
#generate word cloud
set.seed(314)
wordcloud(words = netflix_dtm_d$word, freq = netflix_dtm_d$freq, min.freq = 10,
          max.words=100, random.order=FALSE, rot.per=0.20, 
          colors=brewer.pal(8, "Dark2"))

##play around with shapes



```



```{r}
netflix_vector <- get_sentiment(netflix$description, method="syuzhet")
# see the first row of the vector
head(netflix_vector, n = 10)
# see summary statistics of the vector
summary(netflix_vector)
hist(netflix_vector, col = "blue2", breaks = 20) #normal distribtion 

cor(netflix_vector, netflix$imdb_score)

summary(netflix$imdb_score)
```




