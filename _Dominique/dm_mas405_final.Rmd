---
title: "dm_mas405_final"
author: "Dominique McDonald"
date: '2022-05-18'
output: pdf_document
---

Using the following as a guide: https://www.red-gate.com/simple-talk/databases/sql-server/bi-sql-server/text-mining-and-sentiment-analysis-with-r/




Installing packages for text analysis.
```{r}
# install.packages("tm")  # for text mining
# install.packages("SnowballC") # for text stemming
# install.packages("wordcloud") # word-cloud generator
# install.packages("RColorBrewer") # color palettes
# install.packages("syuzhet") # for sentiment analysis
# install.packages("ggplot2")
```

```{r, warning= FALSE}
#Now load those bad boys


library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
library(RMySQL)
library(dplyr)
library(stringr)
library(DBI)
```





```{r}
#connecting to my db

drv <- dbDriver("MySQL")
xdbsock <- ""


#############
# xdbuser <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_USER")
# xpw     <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_PW")
# xdbname <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_DBNAME")
# xdbhost <- Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_HOST")
# xdbport <- as.integer( Sys.getenv("MAS405_AWS_DOMINIQUE_DB_ROUSER_PORT") )

xdbuser <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_USER")
xpw     <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_PW")
xdbname <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_DBNAME")
xdbhost <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_HOST")
xdbport <- as.integer( Sys.getenv("MAS405_AWS_MY_DB_ADMIN_PORT") )


con <-
dbConnect(
drv,
user=xdbuser,
password=xpw,
dbname=xdbname,
host=xdbhost,
port=xdbport,
unix.sock=xdbsock
)


dbListTables(con)

```

I'm getting rid of all these extra tables. I'll comment these out, but they're here if you're sick of the clutter like me.

```{r}
# 
# #dbListTables(con)
# 
# qry1 <- "DROP TABLE Artists"
# 
# dbGetQuery(con, qry1)
# #dbListTables(con)
# 
# qry1 <- "DROP TABLE Lyrics"
# 
# dbGetQuery(con, qry1)
# #dbListTables(con)
# 
# qry1 <- "DROP TABLE Sonnets"
# dbGetQuery(con, qry1)
# 
# qry1 <- "DROP TABLE sonnets"
# dbGetQuery(con, qry1)
# 
# qry1 <- "DROP TABLE artists_complete "
# dbGetQuery(con, qry1)
# 
# #dbListTables(con)

```



```{r}
#getting the data

#Decided to use OG sonnets instead of pseudo sonnets
#qry1 <- "SELECT * FROM sonnets"

qry1 <- "SELECT * FROM OG"

og <- dbGetQuery(con, qry1)
names(og)

names(og) <- c("row_names", "Sonnets") 
head(og)


#clean up sonnets if you haven't already


og <- og %>% 
  mutate_at("Sonnets", str_replace_all, "â€™", "\'") #the pattern for the special character may vary from computer to computer



```


```{r}


#need to load vector of text objects as a corpus
#VectorSource() interprets each element of a vec as a document
x_text <- Corpus(VectorSource(og$Sonnets))


x_text
```
In the future no need to use content_transformer(). Cleaning the data beforehand seems more logical - could also use base functions that way.
```{r}
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (y , pattern ) gsub(pattern, " ", y))

x_text <- tm_map(x_text, toSpace, "/")
x_text <- tm_map(x_text, toSpace, "@")
x_text <- tm_map(x_text, toSpace, "\\|")

# Convert the text to lower case
#x_text <- tm_map(x_text, content_transformer(tolower)) #we already did this
# Remove numbers
x_text <- tm_map(x_text, removeNumbers)
# Remove english common stopwords
x_text <- tm_map(x_text, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector

x_text <- tm_map(x_text, removeWords, c("thi", "thee", "thou", "may", "still", "thus", "though", "can", "will", "hath", "doth", "thine", "like", "much", "let",
                                        "upon", "from", "dost", "shall", "thy")) 
# Remove punctuations
x_text <- tm_map(x_text, removePunctuation)
# Eliminate extra white spaces
x_text <- tm_map(x_text, stripWhitespace)
# Text stemming - which reduces words to their root form
#x_text <- tm_map(x_text, stemDocument)

x_text #NO DOCUMENTS DROPPED

```
The document term matrix just contains all the words in your "documents" and their frequencies and maybe their stem, gotta check
```{r}
# Build a term-document matrix
x_text_dtm <- TermDocumentMatrix(x_text)

x_text_dtm
#number of total terms is the non sparse entries

mat_dtm <- as.matrix(x_text_dtm)
# mat_dtm
# Sort by decreasing value of frequency
dtm_v <- sort(rowSums(mat_dtm),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 50)


```

```{r}
#generate word cloud
set.seed(314)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.20, 
          colors=brewer.pal(8, "Dark2"))
head(dtm_v)
##play around with shapes

```



```{r}
# Find associations 
findAssocs(x_text_dtm, terms = c("eyes","sweet","love", "heart", "time"), corlimit = 0.4)			




```


```{r}
# Find associations for words that occur at least 50 times
findAssocs(x_text_dtm, terms = findFreqTerms(x_text_dtm, lowfreq = 50), corlimit = 0.40)

```

```{r}

# regular sentiment score using get_sentiment() function and method of your choice
# please note that different methods may have different scales

#### NOTE YOU DO NOT HAVE TO USE A CORPUS FOR THE SYUZHET PACKAGE HERE. THIS IS JUST THE VECTOR OF SONNETS
s_vector <- get_sentiment(og$Sonnets, method="syuzhet") 
# see the first row of the vector
head(s_vector, n = 10)
# see summary statistics of the vector
summary(s_vector)
hist(s_vector, col = "pink", main = "Sentiments Scores for each Sonnet", xlab = "Sentiment Scores") #normal distribution 


```


```{r}

#simple_plot(s_vector) #dont think this will be useful
head(dtm_d)
num_terms <- length(x_text_dtm$i); num_terms #total number of terms - 7611
nTerms(x_text_dtm) # unique terms in sonnets - 3089
head(Terms(x_text_dtm)) #just a list of all the terms that show up

prop_terms<-  dtm_d$freq/num_terms; head(prop_terms)

dtm_prop <- cbind(dtm_d, prop_terms)



```




Looking into other data sets


We have decided not to use the netflix data, but keeping this just in case.
```{r, eval = FALSE}

# qry1 <- "SELECT * FROM netflix"
# 
# netflix <- dbGetQuery(con, qry1)
# 
# head(netflix)
# names(netflix)
# head(netflix$description)#probably more useful
# head(netflix$genres) #not gonna be helpful for us really

```




```{r, eval= FALSE}
#need to load vector of text objects as a corpus
#VectorSource() interprets each element of a vec as a document
# netflix_text <- Corpus(VectorSource(netflix$description))
# 
# 
# netflix_text
```




```{r, eval= FALSE}
# #Replacing "/", "@" and "|" with space
# toSpace <- content_transformer(function (y , pattern ) gsub(pattern, " ", y))
# #to_e <- content_transformer(function (y , pattern ) gsub(pattern, "e", y)) wasnt' helpful
# netflix_text <- tm_map(netflix_text, toSpace, "/")
# netflix_text <- tm_map(netflix_text, toSpace, "@")
# netflix_text <- tm_map(netflix_text, toSpace, "\\|")
# netflix_text <- tm_map(netflix_text, to_, "â€™")
# # Convert the text to lower case
# netflix_text <- tm_map(netflix_text, content_transformer(tolower))
# # Remove numbers
# netflix_text <- tm_map(netflix_text, removeNumbers)
# # Remove english common stopwords
# netflix_text <- tm_map(netflix_text, removeWords, stopwords("english"))
# # Remove your own stop word
# # specify your custom stopwords as a character vector
# 
# netflix_text <- tm_map(netflix_text, removeWords, c("thi", "thee", "thou", "may", "still", "thus", "though", "can", "will", "hath", "doth", "thine", "like", "much", "let",
#                                         "upon", "from", "dost", "shall", "thy")) 
# # Remove punctuations
# netflix_text <- tm_map(netflix_text, removePunctuation)
# # Eliminate extra white spaces
# netflix_text <- tm_map(netflix_text, stripWhitespace)
# # Text stemming - which reduces words to their root form
# #x_text <- tm_map(x_text, stemDocument)
# 
# netflix_text #NO DOCUMENTS DROPPED

```



```{r, eval= FALSE}
# Build a term-document matrix
# netflix_text_dtm <- TermDocumentMatrix(netflix_text)
# netflix_mat_dtm <- as.matrix(netflix_text_dtm)
# #head(netflix_mat_dtm)
# # Sort by decreasing value of frequency
# netflix_dtm_v <- sort(rowSums(netflix_mat_dtm),decreasing=TRUE)
# netflix_dtm_d <- data.frame(word = names(netflix_dtm_v),freq=netflix_dtm_v)
# # Display the top 5 most frequent words
# head(netflix_dtm_d, 50)


```






```{r, eval= FALSE}
# #generate word cloud
# set.seed(314)
# wordcloud(words = netflix_dtm_d$word, freq = netflix_dtm_d$freq, min.freq = 10,
#           max.words=100, random.order=FALSE, rot.per=0.20, 
#           colors=brewer.pal(8, "Dark2"))
# 
# ##play around with shapes



```



```{r, eval= FALSE}
# netflix_vector <- get_sentiment(netflix$description, method="syuzhet")
# # see the first row of the vector
# head(netflix_vector, n = 10)
# # see summary statistics of the vector
# summary(netflix_vector)
# hist(netflix_vector, col = "blue2", breaks = 20) #normal distribtion 
# 
# cor(netflix_vector, netflix$imdb_score)
# 
# summary(netflix$imdb_score)
```

########## WORKING WITH ARTIST DATA NOW #############

This was my code to combine the artists with their complete lyrics. Don't need anymore, but will comment out just in case.
```{r}

 
# #The Lyrics table has the artists' names, but the songs were cutoff. The Artist table has the complete songs, but no names, so I'm gonna split them and combine the right parts
# 
#  qry1 <- "SELECT * FROM Lyrics"
#  lyrics <- dbGetQuery(con, qry1)
# 
#  head(lyrics, n = 2)
#  dim(lyrics)
#  colnames(lyrics) <- c("ID","Artist", "songs")
#  
#  artist_names <- lyrics$Artist
# 
# #View(lyrics)
# 
# 
#  
#  qry1 <- "SELECT * FROM Artists"
#  artists <- dbGetQuery(con, qry1)
#  artist_songs <- artists$`0`
# 
# 
#  artists_complete <- cbind(lyrics$ID, artist_names, artist_songs)
#  artists_complete <- as.data.frame(artists_complete)
#  
#  
#  #THIS TABLE HAS BOTH THE ARTISTS' NAMES AND THE COMPLETE ENTRIES OF THE SONG LYRICS 
#  dbWriteTable(con, "artists_complete", artists_complete)
#  
#  View(artists_complete)
# 
#   
#  ######THERE ARE DUPLICATES IN THE DATA REMEMBER TO COME BACK AND CHANGE THEM! RIGHT NOW I AM JUST FOCUSED ON WRITING THE LOOP#######
# #specifically observations 41 & 42
# 
# #need to look into words within []
# 
# 
# # test<- artist$songs[1]
# # library(stringr)
# # str_extract_all(pattern = "^\\[") #use this to get rid of words in brackets
# 
# 
# library(stringr)
# qry1 <- "SELECT * FROM artists_complete"
# test <- dbGetQuery(con, qry1)
# 
# head(test$artist_songs)
# 
# 
# vec <- str_sub(test$artist_songs[1], start = "\\[", end = "\\]", omit_na = TRUE)
# vec


```


```{r}
#getting final (hopefully) artist data

dbDisconnect(con)    #just in case you're already connected somewhere else

xdbuser <- Sys.getenv("MAS405_AWS_ANGEL_DB_ROUSER_USER")
xpw     <- Sys.getenv("MAS405_AWS_ANGEL_DB_ROUSER_PW")
xdbname <- Sys.getenv("MAS405_AWS_ANGEL_DB_ROUSER_DBNAME")
xdbhost <- Sys.getenv("MAS405_AWS_ANGEL_DB_ROUSER_HOST")
xdbport <- as.integer( Sys.getenv("MAS405_AWS_ANGEL_DB_ROUSER_PORT") )

con <-
dbConnect(
drv,
user=xdbuser,
password=xpw,
dbname=xdbname,
host=xdbhost,
port=xdbport,
unix.sock=xdbsock
)


dbListTables(con)



qry1 <- "SELECT * FROM Artists_noDuplicates"

artists_complete <- dbGetQuery(con, qry1)

#View(artists_complete)

names(artists_complete) <- c("artist_names", "artist_songs") #changing the names to match MG's code (names were different in table from AS's db)

```


```{r}
#using MG's code to clean data. Thanks, Cuata <3

#############
#*sigh* idk why dbWriteTable creates a column of row numbers
#table is loaded with an extra and useless column

#artists_complete <- select(artists_complete,-c(1)) ### I commented this line out bc I didn't write table to my db bc laziness/time constraints - DM

###this code will take all the non-lyrical strings contained in a set of 
#parenthesis and compile it into some vector of lists thing...trust me, you'll see
#
df  <- vector(mode = "list", length = 49)
names(df) <- artists_complete$artist_names

for (i in 1:49) {
  
  df[[i]] <- gsub("[\\(\\)]", "", unlist(regmatches(artists_complete$artist_songs[i],
                                            gregexpr("\\(.*?\\)", 
                                            artists_complete$artist_songs[i]))))
}

#df      #commented this bc it's a lot to print - DM 


#############
###need to input code that will remove the phrases that are not lyrics

#I looked through the lists and roughly 10 artists contained these wannabe lyrics
#Of those 10, about 17 wannabe lyrics were discovered and now must be eliminated

artists_complete <- 
  artists_complete %>% 
  mutate_at("artist_songs", str_remove_all, pattern = c("Verse 1|Chorus 2|Verse 3|Chorus 3|x2|vocal solo|Saxophone solo|fadeout|chorus|Repeat 4 times|bv=|scat singing|4x|x8|x7|x4|2x")) 

#can run this line of code again to verify if the strings are gone
#I would check adele, she had "Verse 1, Chorus 2, Verse 3, and Chorus 3
#you'll find that those phrases are no longer there and what remains is just 
#empty quotations...it's like you can still feel its presence but it's GONE

df2  <- vector(mode = "list", length = 49)
names(df2) <- artists_complete$artist_names

for (i in 1:49) {
  
  df2[[i]] <- gsub("[\\(\\)]", "", unlist(regmatches(artists_complete$artist_songs[i],
                                            gregexpr("\\(.*?\\)", 
                                            artists_complete$artist_songs[i]))))
}

df2[["adele"]]

############
#the last thing to do is to remove unwanted punctuation

#this removes everything inside brackets, brackets included
artists_complete$artist_songs <- gsub("\\[.+?\\]", "", 
                                      artists_complete$artist_songs)

#now with parenthesis includes lyrics being sung...
#this removes all the parenthesis BUT without removing the text inside
artists_complete$artist_songs <- gsub("\\s*\\([^\\)]+\\)","", 
                                        artists_complete$artist_songs)


#This replaces everything that's not alphanumeric signs, space or apostrophe 
#with an empty string...yes please
artists_complete$artist_songs <- gsub("[^[:alnum:][:space:]']", "", 
                                      artists_complete$artist_songs)





```



Gonna write a loop that performs key word extraction on each artist's work. 
Result: - A list containing a df for each artists.
        - Each artist's df will contain: top 10 keywords, frequencies, proportions
        
```{r}
#creating list that will be populated by loop
artist_keyword <- list()
artist_keyword


artist_text <- Corpus(VectorSource(artists_complete[,2])); artist_text

for(i in 1:length(artists_complete[,2])){
 
  kw <- rep(NA, 10)
  frq <- rep(NA, 10)
  prp <- rep(NA, 10)
  temp_mat <- cbind(kw, frq, prp)
  artist_keyword[[i]] <-as.data.frame(temp_mat) # ea. element will be df with 10 rows and 3 columns, chose not to                                                                                           # include artist because it would just be repeated, but could be                                                                                            # useful. discuss???
  colnames(artist_keyword[[i]]) <- c("keyword","frequency", "proportion" )
  
  art_doc <- artist_text[i]
  
  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (y , pattern ) gsub(pattern, " ", y))
  #to_e <- content_transformer(function (y , pattern ) gsub(pattern, "e", y)) wasnt' helpful
  art_doc <- tm_map(art_doc, toSpace, "/")
  art_doc <- tm_map(art_doc, toSpace, "@")
  art_doc <- tm_map(art_doc, toSpace, "\\|")
  #art_doc <- tm_map(art_doc, to_e, "â€™")
  # Convert the text to lower case
  art_doc <- tm_map(art_doc, content_transformer(tolower))
  # Remove numbers
  art_doc <- tm_map(art_doc, removeNumbers)
  # Remove english common stopwords
  art_doc <- tm_map(art_doc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  
  art_doc <- tm_map(art_doc, removeWords, c("aint", "ooh", "thou", "never", "yeah", "hey", "though", "just", "will", "dont", "gonna", "can",                                                            "let", "thing", "every", "cause", "Since", "along",  "always", "many" , "eighteen", "hundred",
                                            "upon", "from", "nah", "aint", "now", "one", "two", "cant", "dont", "wont", "like", "much")) 
  # Remove punctuations
  art_doc <- tm_map(art_doc, removePunctuation)
  # Eliminate extra white spaces
  art_doc <- tm_map(art_doc, stripWhitespace)
  # Text stemming - which reduces words to their root form
  #x_text <- tm_map(art_doc, stemDocument)
  
  #STEMMING DOESNT WORK ON CURSE WORDS
  
  #art_doc #NO DOCUMENTS DROPPED

  art_doc_dtm <- TermDocumentMatrix(art_doc)
  

  #art_doc_dtm
  #number of total terms is the non sparse entries
  
  artist_mat_dtm <- as.matrix(art_doc_dtm)
  # mat_dtm
  # Sort by decreasing value of frequency
  artisit_dtm_v <- sort(rowSums(artist_mat_dtm),decreasing=TRUE)
  artist_dtm_d <- data.frame(word = names(artisit_dtm_v),freq=artisit_dtm_v)
  artist_dtm_d
  
  
  artist_keyword[[i]]$keyword[1:10] <- artist_dtm_d$word[1:10] #populating keywords
  artist_keyword[[i]]$frequency[1:10] <- artist_dtm_d$freq[1:10]; artist_keyword #populating frequencies
  
  art_num_terms <- length(art_doc_dtm$i); art_num_terms #total number of terms
  artist_keyword[[i]]$proportion[1:10] <- artist_dtm_d$freq[1:10]/art_num_terms #populating proportions
  
  
}

names(artist_keyword) <- artists_complete$artist_names
artist_keyword[28] #look at the data frame one at a time otherwise your computer won't like you

dbListTables(con)

artist_keyword[1:45] #run this from your console to see results more easily




```
Since we know who the top artists are I'm gonna write a loop to generate their word clouds 
Amy Whinehouse
Al Green
Bjork
Joni Mitchell
Paul Simon
Beiber
Adele

Double check these artists with Danny's results - I just made the list below from memory
```{r}
#You can probably add a few lines to the loop at the end to write each word cloud to a .png 

top_artists_df <- artists_complete[artists_complete$artist_names %in% c("al-green", "amy-winehouse", "adele", "beiber", "bjork", "cake", "alicia-keys", 
                                                                        "joni-mitchell", "paul-simon"), ]

top_artists_df #check check check

for(i in 1:nrow(top_artists_df)){
  song_doc <- Corpus(VectorSource(top_artists_df[i,2])) #convert the artists songs into document
  
   song_doc <- tm_map(song_doc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
    song_doc <- tm_map(song_doc, removePunctuation)
  
   song_doc <- tm_map(song_doc, removeWords, c("aint", "ooh", "and", "but", "never", "yeah", "hey", "though", "just", "will",       "dont", "gonna", "can", "let", "thing", "every", "cause", "Since", "along",  "always", "many" , "eighteen", "hundred", 
   "upon", "from", "nah", "aint", "now", "one", "two", "cant", "dont", "wont", "like", "much")) 

  
  song_doc_dtm <- TermDocumentMatrix(song_doc)
  
  song_mat_dtm <- as.matrix(song_doc_dtm)
  
  # Sort by decreasing value of frequency
  song_dtm_v <- sort(rowSums(song_mat_dtm),decreasing=TRUE)
  song_dtm_d <- data.frame(word = names(song_dtm_v),freq=song_dtm_v)
  
  
  
  set.seed(314)
wordcloud(words = song_dtm_d$word, freq = song_dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.20, 
          colors=brewer.pal(8, "Dark2"))
  
}

####This loop is having issues with stop words -_____- other than that it works. I will work on it more Sunday or Monday


```

*********** ANDY SUGGESTED USING CLUSTERING TO CHECK SIMILIARITY IN ADDITION TO EUCLIDEAN DISTANCE ************* (???)

- can use clustering for sentiment analysis comparisons

##

```{r}
#using syuzhet package for sentiment analysis

sent_scores<- get_sentiment(artists_complete$artist_songs) #returns overall sentiment value
summary(sent_scores)

artists_8sent <- get_nrc_sentiment(artists_complete$artist_songs) #returns classification into each 8 sentiments + pos/neg
artists_8sent


all_sonnets <- paste(og$Sonnets[1:154], collapse = " ") #the collapse argument allows you to create a single string by concatenating ea element of the vector
all_sonnets

shake_8sent <- get_nrc_sentiment(all_sonnets) #getting shakespeare sentiments

shake_8sent


all_8sent <- rbind(artists_8sent, shake_8sent) #appending shakespeare's sentiments to table with those of other artists

names_8sent <- c(artists_complete$artist_names, "shakespeare") #getting artist names for table
names_8sent

all_8sent <- cbind(names_8sent, all_8sent) #adding names
all_8sent #quick check


#creating a table of proportions instead of frequencies
prop_8sent <- data.frame(matrix(ncol = 9, nrow = 46))

prop_8sent[,1] <- all_8sent$names_8sent


for(i in 1:46){ #this loop divides each element in a row bu the row sum
  
  
  prop_8sent[i,2:9] <- all_8sent[i,2:9]/sum(all_8sent[i,2:9])
  
  }

prop_8sent #another quick checl

```
Now that we have the sentiments in a table kmeans clustering will be straightforward. I will use MG's lovely code for this part :*

```{r}

#identifying optimal number of clusters
k <- 12
vpc <- NULL #vpc here means "variance per cluster"
for (i in 1:k) {
    kfit <- kmeans(prop_8sent[,2:9], i)
    vpc <- c(vpc, kfit$betweenss/kfit$totss)
}
vpc

plot(1:k, vpc, xlab = "# of clusters", ylab = "explained variance")



kfit7 <- kmeans(prop_8sent[,2:9], 7)
kfit8 <- kmeans(prop_8sent[,2:9], 8)  
kfit10 <- kmeans(prop_8sent[,2:9], 10)
kfit12 <- kmeans(prop_8sent[,2:9], 12)


#clustering means were given for each of the eight emotions...
kfit7$centers
kfit8$centers
kfit10$centers
kfit12$centers

#install.packages("factoextra")

library(factoextra)

row.names(prop_8sent) <- all_8sent$names_8sent ###*changing the row names so they will show up as labels for each poin


fviz_cluster(kfit7, data = prop_8sent[,2:9],
             palette = c("deepskyblue", "cyan3", "violet", "pink3", "red", "purple", "green3", "lightpink", "salmon3", "gray", "orange3", "olivedrab3"), 
             geom = c("point", "text"),
             ellipse = F,
             ellipse.type = "t", 
             ggtheme = theme_bw()
             )
fviz_cluster(kfit8, data = prop_8sent[,2:9],
             palette = c("deepskyblue", "cyan3", "violet", "pink3", "red", "purple", "green3", "lightpink", "salmon3", "gray", "orange3", "olivedrab3"), 
             geom = c("point", "text"),
             ellipse = F,
             ellipse.type = "t", 
             ggtheme = theme_bw()
             )

fviz_cluster(kfit10, data = prop_8sent[,2:9],
             palette = c("deepskyblue", "cyan3", "violet", "pink3", "red", "purple", "green3", "lightpink", "salmon3", "gray", "orange3", "olivedrab3"), 
             geom = c("point", "text"),
             ellipse = F,
             ellipse.type = "t", 
             ggtheme = theme_bw()
             )

fviz_cluster(kfit12, data = prop_8sent[,2:9],
             palette = c("deepskyblue", "cyan3", "violet", "pink3", "red", "purple", "green3", "lightpink", "salmon3", "gray", "orange", "olivedrab3"), 
             geom = c("point", "text"),
             ellipse = F,
             ellipse.type = "t", 
             ggtheme = theme_bw()
             )


test <- cbind (all_8sent, kfit$cluster) #Appended the clusters to all artists sentiments to get table of artist in same group
#need to check which cluster shakespeare is in each time, because different groups might be formed each time you run the cluster analysis

# test[test$`kfit$cluster` == 4,] #these are the artists in shakespeare's cluster

```
















